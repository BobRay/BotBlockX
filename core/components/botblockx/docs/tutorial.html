<p>BotBlockX is based on Alex Kemp's <a href="http://download.modem-help.co.uk/non-modem/PHP/Rogue-Bot-Blocking/">classic bot-blocking routine</a>. I have rewritten it for MODX Revolution and added a few tweaks, but the heart of it is Alex's original code.</p>

<p>The default install of this plugin will block badly behaved bots (both slow and fast), while leaving good bots like Google alone. If you want to allow slow scrapers to grab your content (some regular users download whole sites to use for offline reference), you can disable the slow sraper check.</p>

<p>BotBlockX creates a lot of files in the block/ directory, but they are all zero-length files so they shouldn't count as resources on your site. This is possible because of Alex's ingenious use of the files' modification time and access time to store information about the visitor's behavior. Both the block/ directory and the log/ directory are placed in the MODX root to speed up access time.</p>

<h3>How it Works</h3>

<p>When a user visits the site, the plugin (connected to both OnPageNotFound and OnHandleRequest), creates an "IP file," the name of which is a substring of the MD5 hash of their IP (the length of which is determined by the &amp;ip_length property). The duration of their visit, and the number of hits during the specified interval is stored in the file's access time and modification time.</p>

<p>Note that almost all "good" bots are whitelisted by IP range and can do whatever they want. In future versions, this will be an option setting for people who want to block all bots based on behavior alone.</p>

<p><b>Slow Scrapers</b> &mdash; If a user makes more than &amp;total_visits requests in the seconds defined by &amp;start_over_secs, they've violated the slow scraper rules and will receive a message telling them that they'll have to wait to see another page. With the default settings, they can make 1500 requests in 12 hours and violators will have to wait about 12 hours before getting access to any page at the site.</p>

<p><b>Fast Scrapers</b> &mdash; If a user makes more than &amp;max_visits requests in the seconds defined by &amp;interval, they've violated the fast scraper rules and will receive a message telling them that they'll have to wait to see another page. With the default settings, they can make 14 requests in 7 seconds. The wait penalty varies with the frequency of their requests. It's hard to test, but I think it's usually around 60-120 seconds.</p>

<p>To control good bots, place these lines in your robots.txt file in the MODX root directory. Leave a space above and below the lines:</p>

<pre>
User-agent: *
Crawl-delay: 90
Crawl-Delay: 90
</pre>

<h3>Testing</h3>
<p>Note that it's almost impossible to trigger BotBlockX with a standard browser and the default settings. If you set &amp;total_visits to 5 and &amp;secs_to_start_over to 20, you can trigger the slow scraper warning, but you'll be blocking regular visitors to your site (don't forget to set them back). To trigger the fast scraper block, you'll need a bot program that can make more than one request per second.</p>

<p>If you just want to see if the program is working at all, go to the files tab and expand the &quot;block/&quot; directory at the root of the site. You should see a growing number of zero-length IP files there. If any bots have been blocked, you'll see entries in the log file in the &quot;log/&quot; directory, also in the MODx root directory.  </p>




<h3>Installing BotBlockX</h3>

<p>Go to System | Package Management and click on the &quot;Download Extras&quot; button. Put &quot;BotBlockX&quot; in the search box and press &quot;Enter&quot;. Click on the &quot;Download&quot; button next to BotBlockX on the right side of the grid. Wait until the &quot;Download&quot; button changes to &quot;Downloaded&quot; and click on the &quot;Finish&quot; button. The BotBlockX package should appear in the Package Manager grid. Clock on the &quot;Install&quot; button and respond to the forms presented. That's it. Once the install finishes, bot-blocking will begin immediately.</p>

<p>Some servers do not have both the fileatime() and filemtime() functions enabled. The BotBlockX package performs a test for this during the install and aborts the install if both functions are not working correctly. If that happens, you'll see a message telling you about it and will have to talk things over with your host.</p>

<p>Another possible issue is that some servers reset the access time of all files when doing their daily backups (usually around 2:30 a.m.) If this is the case, some visitors may be blocked for a short period after the backup. I think that most servers don't do this, but if yours does, there is a fix for it in the comments of the code at Alex's site.</p>

<h3>BotBlockXLog</h3>
<p>When a bot is blocked, an entry is written to the BotBlockX log (inside the log/ directory below the MODX root directory). The log entry will the IP, host name, date/time, useragent, and the the type of scraper (slow or fast). The fields are separated by back-ticks. Remember that there will be an entry for every request made by a blocked bot, so there will be a lot of duplicates, especially with stupid fast scrapers. The log is limited to 1,000 entries (otherwise it would be monstrous). New entries will be added at the top and old ones will scroll off the bottom. </p>

<h3>Properties</h3>

<p>The default property settings have evolved over a number of years and I don't recommend changing most of them except for testing. You can change the option to show the appeal and you can change the ip_length property if your site is very popular or unpopular, but the default ip_length of 3 is a fairly good compromise for most sites.</p>

<p>Note that the &amp;max_visits property value *must* be greater than the setting for the &amp;interval property.</p>

<p>Feel free to rewrite the Tpl chunks to meet your needs, but be careful not to have any blank lines above or below the HTML code or you may trigger a "headers already sent" error. If you do rewrite the Tpl chunks or change any other properties, be sure to create a property set to hold the alternate values so your settings won't be overwritten when you upgrade BotBlockX. Create the property set on the &quot;Properties&quot; tab of the plugin, then go to the &quot;System Events&quot; tab and specify your property set in the &quot;Property Set&quot; column next to the two checked events (OnPageNotFound and OnHandleRequest). Don't forget to save *both* the property set and the plugin.</p>

<h3>Roadmap</h3>

<p>The following features are planned for future releases (not necessarily in this order):</p>
<ul>
    <li>Make the good-bot whitelist an option</li>
    <li>Make the log size an option</li>
    <li>Separate plugin to log page-not-found requests</li>
    <li>Increased penalty for page-not-found requests</li>
    <li>Severe penalty for bots looking for the &quot;reflect&amp; snippet</li>
    <li>Reporting page for the log</li>
</ul>

<p>&nbsp;</p>