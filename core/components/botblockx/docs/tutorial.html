<p>BotBlockX is based on Alex Kemp's <a href="http://download.modem-help.co.uk/non-modem/PHP/Rogue-Bot-Blocking/">classic bot-blocking routine</a>. I have rewritten it for MODX Revolution and added a few tweaks, but the heart of it is Alex's original code.</p>

<p>The default install of this plugin will block badly behaved bots (both slow and fast), while leaving good bots like Google alone. If you want to allow slow scrapers to grab your content (some regular users download whole sites to use for offline reference), you can disable the slow scraper check by changing the &amp;total_visits property to 'none'.</p>

<p>BotBlockX creates a <i>lot</i> of files in the block/ directory, but they are all zero-length files so they shouldn't count as resources on your site. This is possible because of Alex's ingenious use of the files' modification time and access time to store information about the visitor's behavior. Both the block/ directory and the blocklogs/ directory are placed directly under the MODX core directory to speed up access time.</p>

<h3>How it Works</h3>

<p>When a user visits the site, the plugin (connected to both OnPageNotFound and OnHandleRequest), creates an "IP file," the name of which is a substring of the MD5 hash of their IP (the length of which is determined by the &amp;ip_length property). The duration of their visit, and the number of hits during the specified interval is stored in the file's access time and modification time.</p>

<p>Note that almost all "good" bots are whitelisted by IP range and can do whatever they want. You can disable this (and significantly speed up the plugin) by changing the &amp;use_whitelist property of the plugin, but you may end up blocking some legitimate, but badly behaved, search spiders.</p>

<p>To control good bots, place these lines in your robots.txt file in the MODX root directory. Leave a space above and below the lines:</p>

<pre>
User-agent: *
Crawl-delay: 90
Crawl-Delay: 90
</pre>

<p>If the lines above are in your robots.txt file, no well-behaved bot should be caught by the fast scraper code.</p>

<p><b>Slow Scrapers</b> &mdash; If a user makes more than &amp;total_visits requests in the seconds defined by &amp;start_over_secs, they've violated the slow scraper rules and will receive a message telling them that they'll have to wait to see another page. With the default settings, they can make 1500 requests in 12 hours and violators will have to wait about 12 hours before getting access to any page at the site.</p>

<p><b>Fast Scrapers</b> &mdash; If a user makes more than &amp;max_visits requests in the seconds defined by &amp;interval, they've violated the fast scraper rules and will receive a message telling them that they'll have to wait to see another page. With the default settings, they can make 14 requests every 7 seconds. The wait penalty varies with the frequency of their requests. It's hard to test, but I think it's usually around 60-120 seconds.</p>


<h3>Testing</h3>
<p>Note that it's almost impossible to trigger BotBlockX with a standard browser and the default settings. If you set &amp;total_visits to 5 and &amp;secs_to_start_over to 20, you can trigger the slow scraper warning, but you'll be blocking regular visitors to your site (don't forget to set them back).</p>

<p>Note that once you've blocked yourself, you'll have to either wait 12 hours to see the front end again, or delete the IP file in the core/block/ directory. Since the filenames are hashed, you'll probably have to delete all of them, so try this test before there are a lot of files there.</p>

<p>To trigger the fast scraper block, you'll need a bot program that can make more than one request per second.</p>

<p>If you just want to see if the program is working at all, go to the files tab and expand the &quot;block/&quot; directory at the root of the site. You should see a growing number of zero-length IP files there. If any bots have been blocked, you'll see entries in the log file in the &quot;log/&quot; directory, also in the MODx root directory.  </p>


<h3>Installing BotBlockX</h3>

<p>Go to System | Package Management and click on the &quot;Download Extras&quot; button. Put &quot;BotBlockX&quot; in the search box and press &quot;Enter&quot;. Click on the &quot;Download&quot; button next to BotBlockX on the right side of the grid. Wait until the &quot;Download&quot; button changes to &quot;Downloaded&quot; and click on the &quot;Finish&quot; button.</p>

<p>The BotBlockX package should appear in the Package Manager grid. Clock on the &quot;Install&quot; button and respond to the forms presented. That's it. Once the install finishes, bot-blocking will begin immediately. The LogPageNotFound plugin and the ReflectBlock plugin will be installed, but these are disabled by default. You'll also get the three snippet/resource pairs that will let you view the logs created by the three plugins.</p>

<p>Some servers do not have both the fileatime() and filemtime() functions enabled. The BotBlockX package performs a test for this during the install and aborts the install if both functions are not working correctly. If that happens, you'll see a message telling you about it and will have to talk things over with your host.</p>

<p>Another possible issue is that some servers reset the access time of all files when doing their daily backups (usually around 2:30 a.m.) If this is the case, some visitors may be blocked for a short period after the backup. I think that most servers don't do this, but if yours does, there is a fix for it in the comments of the code at Alex's site.</p>

<h3>BotBlockXLog</h3>
<p>When a bot is blocked, an entry is written to the ipblock.log file (inside the blocklogs/ directory below the MODX core directory). The log entry will the IP, host name, date/time, useragent, and the the type of scraper (slow or fast). The fields are separated by back-ticks.</p>

<p>Remember that there will be an entry for every request made by a blocked bot, so there will be a lot of duplicates, especially with stupid fast scrapers. The log is limited to 1,000 entries (otherwise it would be monstrous). New entries will be added at the top and old ones will scroll off the bottom. </p>

<h3>Properties</h3>

<p>The default property settings have evolved over a number of years and I don't recommend changing most of them except for testing. You can change the &amp;ip_length property if your site is very popular or unpopular, but the default &amp;ip_length of 3 is a fairly good compromise for most sites. And you can change the &amp;total_visits limit for slow scrapers if you site is larger or smaller than usual, but remember that a visit to a single page can generate more than one request. You can set &amp;total_visits to 'none' to disable the slow scraper block.</p>


<p>To change the other properties, and the content of the messages sent to violators, you need to edit the code of the plugin. I originally had a number of Tpl chunks and many properties to control the plugin, but the speed penalty was too severe.</p>

<p>Note that the &amp;max_visits property value *must* be greater than the setting for the &amp;interval property.</p>


<h3>ReflectBlock Plugin</h3>

<p>This plugin is disabled when the package is installed. To enable it, you need to edit the plugin, uncheck the &quot;Plugin Disabled&quot; checkbox, and save the plugin.</p>

<p>You'll probably be amazed to find how many evildoers are visiting your site looking for the old reflect snippet, even though it has been gone for some time. If you upgraded from an older version of MODX, however, it could still exist on your site and it's a serious security vulnerability.</p>

<p>Even if it's gone, it's annoying to have people hammering away and using bandwidth trying to find it. They'll get directed to your error page, but this plugin stops them before that. It only executes when OnPageNotFound is triggered, so it won't cause any trouble if you happen to have a URL containing the word &quot;reflect&quot;.</p>

<h3>LogPageNotFound Plugin</h3>

<p>This plugin is disabled when the package is installed. To enable it, you need to edit the plugin, uncheck the &quot;Plugin Disabled&quot; checkbox, and save the plugin.</p>

<p>This is a very handy plugin in general because it will log every request for a page that isn't found. If possible, it will report the URL, Time, IP, Host, User Agent, and referer. The referer is the page the user came from, but it will be empty if the visit is from a bot or the user typed the URL rather than following a link.</p>

<p>You will almost certainly find that there are links out there for pages you've changed the alias of and possibly links at your own site that no longer work. You'll also see evildoers looking for a variety of security holes trying to access files and directories that never existed.</p>

<p>If you have the Reflect Block plugin enabled, you won't see those requests in the Page Not Found log. If it's disabled, you'll see plenty of them if your site contains the word &quot;MODX&quot;.</p>

<p>When you see legitimate requests in this log, you can make them go away with either rewrite rules in .htaccess or weblinks on the site.</p>

<p>Note that if you have an Adsense site, every page-not-found request will be followed by one or more visits from the GoogleBot looking for the same URL. You'll also see the GoogleBot failing to find the unpublished report pages when you access them.</p>

<p>You can prevent some of that with these lines in your robots.txt file. Adjust the file suffix as necessary. Note that the misspelling of reflect is intentional.</p>

<pre>
    User-agent: Mediapartners-Google*
    Disallow: page-not-found-log-report.html
    Disallow: reflect
    Disallow: bot-block-report.html
    Disallow: reflct-block-log-report.html
</pre>

<h3>Reports</h3>
<p>Three report snippets and three resources to execute them are included in the package. The resources are unpublished and hidden from menus by default, but you can still view the logs by previewing them from the Manager when you are logged in as a Super User.</p>

<p>There is one pair to view the contents of each of the following logs:</p>
<ul>
    <li>Block Log -- Log showing blocked fast and slow scrapers</li>
    <li>Reflect Block Log -- Log showing reflect violators</li>
    <li>Page Not Found Log -- Log showing all page-not-found requests</li>
</ul>

<p>Each log snippet has two properties: &amp;table_width (to set the width of the table) and &amp;cell_width (to set the width of each cell). Feel free to adjust them to meet your needs.</p>

<p>If you see serious repeat offenders in any of the three logs, you can block them by IP with code like this in your .htaccess file (using their actual IPs):</p>
<pre>
    order allow,deny
    deny from 127.0.0.1
    deny from 127.0.0.2
    deny from 127.0.0.3
    allow from all
</pre>
<p>Blocking users in the .htaccess file is extremely fast and it stops the users dead before they even reach the site. It's not all that practical, however, since people can spoof IPs, there are zillions of evildoers out there, and the request can be from an IP that might later be assigned to a legitimate user. Be sure to note the host before adding an IP block. You don't want to block the GoogleBot or yourself. The User Agent can be helpful here, but many evildoers will fake the User Agent and pretend to be the GoogleBot.</p>

<p>&nbsp;</p>